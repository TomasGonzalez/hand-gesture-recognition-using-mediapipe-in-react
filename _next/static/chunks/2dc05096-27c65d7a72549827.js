"use strict";(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[864],{6014:function(t,e,i){i.d(e,{$p:function(){return f},U7:function(){return E},_0:function(){return T},lx:function(){return z},nd:function(){return A}});var r=i(9296),s=i(8819),n=i(9840),l=i(8090),a=i(4079),u=i(163),h=i(588),o=i(2599),c=i(539),p=i(2931),g=i(6040),d=i(7538),I=i(1653),C=i(9897);/**
 * @license
 * Copyright 2018 Google LLC
 *
 * Use of this source code is governed by an MIT-style
 * license that can be found in the LICENSE file or at
 * https://opensource.org/licenses/MIT.
 * =============================================================================
 */ function z(t,e,i,r){if(Array.isArray(t)){if(null!=e||null!=i)throw new h.nu("When inputs is an array, neither initialState or constants should be provided");null!=r&&(i=t.slice(t.length-r,t.length),t=t.slice(0,t.length-r)),t.length>1&&(e=t.slice(1,t.length)),t=t[0]}function s(t){return null==t||Array.isArray(t)?t:[t]}return e=s(e),i=s(i),{inputs:t,initialState:e,constants:i}}function A(t,e,i,s=!1,n,l,a=!1,u=!1){return r.lub(()=>{let o=e.shape.length;if(o<3)throw new h.nu(`Input should be at least 3D, but is ${o}D.`);let c=[1,0].concat(g.w6(2,o));if(e=r.p4s(e,c),null!=l)throw new h.nj("The rnn() functoin of the deeplearn.js backend does not support constants yet.");a&&console.warn("Backend rnn(): the unroll = true option is not applicable to the imperative deeplearn.js backend."),null!=n&&((n=r.pju(r.pju(n,"bool"),"float32")).rank===o-1&&(n=r.dt4(n,-1)),n=r.p4s(n,c)),s&&(e=r.GYS(e,0),null!=n&&(n=r.GYS(n,0)));let p=[],d,I=i,C=e.shape[0],z=r.HHK(e),A;null!=n&&(A=r.HHK(n));for(let f=0;f<C;++f){let k=z[f],S=r.lub(()=>t(k,I));if(null==n)d=S[0],I=S[1];else{let b=r.lub(()=>{let t=A[f],e=r.luU(r.JpU(t),t),i=r.IHx(r.dC7(S[0],t),r.dC7(I[0],e)),s=I.map((i,s)=>r.IHx(r.dC7(S[1][s],t),r.dC7(i,e)));return{output:i,newStates:s}});d=b.output,I=b.newStates}u&&p.push(d)}let R;return u&&(R=r.knu(p,1)),[d,R,I]})}class f extends u.mh{constructor(t){super(t);let e;if(null==t.cell)throw new h.nu("cell property is missing for the constructor of RNN.");if(null==(e=Array.isArray(t.cell)?new D({cells:t.cell}):t.cell).stateSize)throw new h.nu("The RNN cell should have an attribute `stateSize` (tuple of integers, one integer per RNN state).");this.cell=e,this.returnSequences=null!=t.returnSequences&&t.returnSequences,this.returnState=null!=t.returnState&&t.returnState,this.goBackwards=null!=t.goBackwards&&t.goBackwards,this._stateful=null!=t.stateful&&t.stateful,this.unroll=null!=t.unroll&&t.unroll,this.supportsMasking=!0,this.inputSpec=[new u.Zg({ndim:3})],this.stateSpec=null,this.states_=null,this.numConstants=null,this.keptStates=[]}getStates(){if(null!=this.states_)return this.states_;{let t=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1;return g.w6(0,t).map(t=>null)}}setStates(t){this.states_=t}computeOutputShape(t){(0,d.XO)(t)&&(t=t[0]);let e=this.cell.stateSize;Array.isArray(e)||(e=[e]);let i=e[0],r;if(r=this.returnSequences?[t[0],t[1],i]:[t[0],i],!this.returnState)return r;{let s=[];for(let n of e)s.push([t[0],n]);return[r].concat(s)}}computeMask(t,e){return r.lub(()=>{Array.isArray(e)&&(e=e[0]);let t=this.returnSequences?e:null;if(!this.returnState)return t;{let i=this.states.map(t=>null);return[t].concat(i)}})}get states(){if(null!=this.states_)return this.states_;{let t=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1,e=[];for(let i=0;i<t;++i)e.push(null);return e}}set states(t){this.states_=t}build(t){if(null!=this.numConstants)throw new h.nj("Constants support is not implemented in RNN yet.");(0,d.XO)(t)&&(t=t[0]);let e=this.stateful?t[0]:null,i=t.slice(2);this.inputSpec[0]=new u.Zg({shape:[e,null,...i]});let s=[t[0]].concat(t.slice(2));this.cell.build(s);let n;if(n=Array.isArray(this.cell.stateSize)?this.cell.stateSize:[this.cell.stateSize],null!=this.stateSpec){if(!r.D5U.arraysEqual(this.stateSpec.map(t=>t.shape[t.shape.length-1]),n))throw new h.nu(`An initialState was passed that is not compatible with cell.stateSize. Received stateSpec=${this.stateSpec}; However cell.stateSize is ${this.cell.stateSize}`)}else this.stateSpec=n.map(t=>new u.Zg({shape:[null,t]}));this.stateful&&this.resetStates()}resetStates(t,e=!1){(0,r.lub)(()=>{if(!this.stateful)throw new h.j1("Cannot call resetStates() on an RNN Layer that is not stateful.");let i=this.inputSpec[0].shape[0];if(null==i)throw new h.nu("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.states_)Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map(t=>r.lls([i,t])):this.states_=[r.lls([i,this.cell.stateSize])];else if(null==t)r.B90(this.states_),null!=this.keptStates&&(r.B90(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map(t=>r.lls([i,t])):this.states_[0]=r.lls([i,this.cell.stateSize]);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new h.nu(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: ${t}`);!0===e?this.keptStates.push(this.states_.slice()):r.B90(this.states_);for(let s=0;s<this.states_.length;++s){let n=t[s],l=Array.isArray(this.cell.stateSize)?this.cell.stateSize[s]:this.cell.stateSize,a=[i,l];if(!r.D5U.arraysEqual(n.shape,a))throw new h.nu(`State ${s} is incompatible with layer ${this.name}: expected shape=${a}, received shape=${n.shape}`);this.states_[s]=n}}this.states_=this.states_.map(t=>r.CnY(t.clone()))})}apply(t,e){let i=null==e?null:e.initialState,r=null==e?null:e.constants;null==e&&(e={});let s=z(t,i,r,this.numConstants);t=s.inputs,i=s.initialState,r=s.constants;let n=[],l=[];if(null!=i){for(let a of(e.initialState=i,n=n.concat(i),this.stateSpec=[],i))this.stateSpec.push(new u.Zg({shape:a.shape}));l=l.concat(this.stateSpec)}null!=r&&(e.constants=r,n=n.concat(r),this.numConstants=r.length);let h=n[0]instanceof u.Iy;if(!h)return super.apply(t,e);{let o=[t].concat(n),c=this.inputSpec.concat(l),p=this.inputSpec;this.inputSpec=c;let g=super.apply(o,e);return this.inputSpec=p,g}}call(t,e){return(0,r.lub)(()=>{let i=null==e?null:e.mask,r=null==e?null:e.training,s=null==e?null:e.initialState;t=(0,d.nQ)(t),null==s&&(s=this.stateful?this.states_:this.getInitialState(t));let n=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1;if(s.length!==n)throw new h.nu(`RNN Layer has ${n} state(s) but was passed ${s.length} initial state(s).`);this.unroll&&console.warn("Ignoring unroll = true for RNN layer, due to imperative backend.");let l={training:r},a=(t,e)=>{let i=this.cell.call([t].concat(e),l);return[i[0],i.slice(1)]},u=A(a,t,s,this.goBackwards,i,null,this.unroll,this.returnSequences),o=u[0],c=u[1],p=u[2];this.stateful&&this.resetStates(p,r);let g=this.returnSequences?c:o;return this.returnState?[g].concat(p):g})}getInitialState(t){return(0,r.lub)(()=>{let e=r.lls(t.shape);return(e=r.Smz(e,[1,2]),e=n.dt(e),Array.isArray(this.cell.stateSize))?this.cell.stateSize.map(t=>t>1?n.Gg(e,[1,t]):e):this.cell.stateSize>1?[n.Gg(e,[1,this.cell.stateSize])]:[e]})}get trainableWeights(){return this.trainable?this.cell.trainableWeights:[]}get nonTrainableWeights(){return this.trainable?this.cell.nonTrainableWeights:this.cell.weights}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.cell&&this.cell.setFastWeightInitDuringBuild(t)}getConfig(){let t=super.getConfig(),e={returnSequences:this.returnSequences,returnState:this.returnState,goBackwards:this.goBackwards,stateful:this.stateful,unroll:this.unroll};null!=this.numConstants&&(e.numConstants=this.numConstants);let i=this.cell.getConfig();return this.getClassName()===f.className&&(e.cell={className:this.cell.getClassName(),config:i}),Object.assign({},i,t,e)}static fromConfig(t,e,i={}){let r=e.cell,s=(0,C.v)(r,i);return new t(Object.assign(e,{cell:s}))}}f.className="RNN",r.m7h.registerClass(f);class k extends u.mh{}class S extends k{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,(0,p.iQ)(this.units,"units"),this.activation=(0,s.aI)(null==t.activation?this.DEFAULT_ACTIVATION:t.activation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=(0,o.L5)(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=(0,o.L5)(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=(0,o.L5)(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=(0,c.EC)(t.kernelRegularizer),this.recurrentRegularizer=(0,c.EC)(t.recurrentRegularizer),this.biasRegularizer=(0,c.EC)(t.biasRegularizer),this.kernelConstraint=(0,a.Ad)(t.kernelConstraint),this.recurrentConstraint=(0,a.Ad)(t.recurrentConstraint),this.biasConstraint=(0,a.Ad)(t.biasConstraint),this.dropout=g.VV([1,g.Fp([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=g.VV([1,g.Fp([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.dropoutFunc=t.dropoutFunc,this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){t=(0,d.Wf)(t),this.kernel=this.addWeight("kernel",[t[t.length-1],this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,e){return(0,r.lub)(()=>{if(2!==t.length)throw new h.nu(`SimpleRNNCell expects 2 input Tensors, got ${t.length}.`);let i=t[1];t=t[0];let s=null!=e.training&&e.training;0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=T({ones:()=>r.JpU(t),rate:this.dropout,training:s,dropoutFunc:this.dropoutFunc})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=T({ones:()=>r.JpU(i),rate:this.recurrentDropout,training:s,dropoutFunc:this.dropoutFunc}));let l,a=this.dropoutMask,u=this.recurrentDropoutMask;l=null!=a?n.AK(r.dC7(t,a),this.kernel.read()):n.AK(t,this.kernel.read()),null!=this.bias&&(l=n.a2(l,this.bias.read())),null!=u&&(i=r.dC7(i,u));let o=r.IHx(l,n.AK(i,this.recurrentKernel.read()));return null!=this.activation&&(o=this.activation.apply(o)),[o,o]})}getConfig(){let t=super.getConfig(),e={units:this.units,activation:(0,s.GD)(this.activation),useBias:this.useBias,kernelInitializer:(0,o.Cx)(this.kernelInitializer),recurrentInitializer:(0,o.Cx)(this.recurrentInitializer),biasInitializer:(0,o.Cx)(this.biasInitializer),kernelRegularizer:(0,c.SG)(this.kernelRegularizer),recurrentRegularizer:(0,c.SG)(this.recurrentRegularizer),biasRegularizer:(0,c.SG)(this.biasRegularizer),activityRegularizer:(0,c.SG)(this.activityRegularizer),kernelConstraint:(0,a.xF)(this.kernelConstraint),recurrentConstraint:(0,a.xF)(this.recurrentConstraint),biasConstraint:(0,a.xF)(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout};return Object.assign({},t,e)}}S.className="SimpleRNNCell",r.m7h.registerClass(S);class b extends f{constructor(t){t.cell=new S(t),super(t)}call(t,e){return(0,r.lub)(()=>{null!=this.cell.dropoutMask&&(r.B90(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(r.B90(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);let i=null==e?null:e.mask,s=null==e?null:e.training,n=null==e?null:e.initialState;return super.call(t,{mask:i,training:s,initialState:n})})}static fromConfig(t,e){return new t(e)}}b.className="SimpleRNN",r.m7h.registerClass(b);class R extends k{constructor(t){if(super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",t.resetAfter)throw new h.nu("GRUCell does not support reset_after parameter set to true.");this.units=t.units,(0,p.iQ)(this.units,"units"),this.activation=(0,s.aI)(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=(0,s.aI)(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=(0,o.L5)(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=(0,o.L5)(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=(0,o.L5)(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=(0,c.EC)(t.kernelRegularizer),this.recurrentRegularizer=(0,c.EC)(t.recurrentRegularizer),this.biasRegularizer=(0,c.EC)(t.biasRegularizer),this.kernelConstraint=(0,a.Ad)(t.kernelConstraint),this.recurrentConstraint=(0,a.Ad)(t.recurrentConstraint),this.biasConstraint=(0,a.Ad)(t.biasConstraint),this.dropout=g.VV([1,g.Fp([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=g.VV([1,g.Fp([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.dropoutFunc=t.dropoutFunc,this.implementation=t.implementation,this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){t=(0,d.Wf)(t);let e=t[t.length-1];this.kernel=this.addWeight("kernel",[e,3*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,3*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[3*this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,e){return(0,r.lub)(()=>{if(2!==t.length)throw new h.nu(`GRUCell expects 2 input Tensors (inputs, h, c), got ${t.length}.`);let i=null!=e.training&&e.training,s=t[1];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=T({ones:()=>r.JpU(t),rate:this.dropout,training:i,count:3,dropoutFunc:this.dropoutFunc})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=T({ones:()=>r.JpU(s),rate:this.recurrentDropout,training:i,count:3,dropoutFunc:this.dropoutFunc}));let l=this.dropoutMask,a=this.recurrentDropoutMask,u,o,c;0<this.dropout&&this.dropout<1&&(t=r.dC7(t,l[0]));let p=n.AK(t,this.kernel.read());this.useBias&&(p=n.a2(p,this.bias.read())),0<this.recurrentDropout&&this.recurrentDropout<1&&(s=r.dC7(s,a[0]));let g=this.recurrentKernel.read(),[d,I]=r.Vl2(g,[2*this.units,this.units],g.rank-1),C=n.AK(s,d),[z,A,f]=r.Vl2(p,3,p.rank-1),[k,S]=r.Vl2(C,2,C.rank-1);u=this.recurrentActivation.apply(r.IHx(z,k)),o=this.recurrentActivation.apply(r.IHx(A,S));let b=n.AK(r.dC7(o,s),I);c=this.activation.apply(r.IHx(f,b));let R=r.IHx(r.dC7(u,s),r.dC7(r.IHx(1,r.W76(u)),c));return[R,R]})}getConfig(){let t=super.getConfig(),e={units:this.units,activation:(0,s.GD)(this.activation),recurrentActivation:(0,s.GD)(this.recurrentActivation),useBias:this.useBias,kernelInitializer:(0,o.Cx)(this.kernelInitializer),recurrentInitializer:(0,o.Cx)(this.recurrentInitializer),biasInitializer:(0,o.Cx)(this.biasInitializer),kernelRegularizer:(0,c.SG)(this.kernelRegularizer),recurrentRegularizer:(0,c.SG)(this.recurrentRegularizer),biasRegularizer:(0,c.SG)(this.biasRegularizer),activityRegularizer:(0,c.SG)(this.activityRegularizer),kernelConstraint:(0,a.xF)(this.kernelConstraint),recurrentConstraint:(0,a.xF)(this.recurrentConstraint),biasConstraint:(0,a.xF)(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation,resetAfter:!1};return Object.assign({},t,e)}}R.className="GRUCell",r.m7h.registerClass(R);class m extends f{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new R(t),super(t)}call(t,e){return(0,r.lub)(()=>{null!=this.cell.dropoutMask&&(r.B90(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(r.B90(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);let i=null==e?null:e.mask,s=null==e?null:e.training,n=null==e?null:e.initialState;return super.call(t,{mask:i,training:s,initialState:n})})}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}m.className="GRU",r.m7h.registerClass(m);class E extends k{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,(0,p.iQ)(this.units,"units"),this.activation=(0,s.aI)(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=(0,s.aI)(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=(0,o.L5)(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=(0,o.L5)(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=(0,o.L5)(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.unitForgetBias=t.unitForgetBias,this.kernelRegularizer=(0,c.EC)(t.kernelRegularizer),this.recurrentRegularizer=(0,c.EC)(t.recurrentRegularizer),this.biasRegularizer=(0,c.EC)(t.biasRegularizer),this.kernelConstraint=(0,a.Ad)(t.kernelConstraint),this.recurrentConstraint=(0,a.Ad)(t.recurrentConstraint),this.biasConstraint=(0,a.Ad)(t.biasConstraint),this.dropout=g.VV([1,g.Fp([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=g.VV([1,g.Fp([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.dropoutFunc=t.dropoutFunc,this.implementation=t.implementation,this.stateSize=[this.units,this.units],this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){var e;t=(0,d.Wf)(t);let i=t[t.length-1];this.kernel=this.addWeight("kernel",[i,4*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,4*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint);let r;if(this.useBias){if(this.unitForgetBias){let s=this.biasInitializer,l=this.units;r=new((e=class extends o.m7{apply(t,e){let i=s.apply([l]),r=new o.M6().apply([l]),a=s.apply([2*l]);return n.GZ(n.GZ(i,r),a)}}).className="CustomInit",e)}else r=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.units],null,r,this.biasRegularizer,!0,this.biasConstraint)}else this.bias=null;this.built=!0}call(t,e){return(0,r.lub)(()=>{let i=null!=e.training&&e.training;if(3!==t.length)throw new h.nu(`LSTMCell expects 3 input Tensors (inputs, h, c), got ${t.length}.`);let s=t[1],l=t[2];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=T({ones:()=>r.JpU(t),rate:this.dropout,training:i,count:4,dropoutFunc:this.dropoutFunc})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=T({ones:()=>r.JpU(s),rate:this.recurrentDropout,training:i,count:4,dropoutFunc:this.dropoutFunc}));let a=this.dropoutMask,u=this.recurrentDropoutMask,o,c,p,g;0<this.dropout&&this.dropout<1&&(t=r.dC7(t,a[0]));let d=n.AK(t,this.kernel.read());0<this.recurrentDropout&&this.recurrentDropout<1&&(s=r.dC7(s,u[0])),d=r.IHx(d,n.AK(s,this.recurrentKernel.read())),this.useBias&&(d=n.a2(d,this.bias.read()));let[I,C,z,A]=r.Vl2(d,4,d.rank-1);o=this.recurrentActivation.apply(I),c=this.recurrentActivation.apply(C),p=r.IHx(r.dC7(c,l),r.dC7(o,this.activation.apply(z))),g=this.recurrentActivation.apply(A);let f=r.dC7(g,this.activation.apply(p));return[f,f,p]})}getConfig(){let t=super.getConfig(),e={units:this.units,activation:(0,s.GD)(this.activation),recurrentActivation:(0,s.GD)(this.recurrentActivation),useBias:this.useBias,kernelInitializer:(0,o.Cx)(this.kernelInitializer),recurrentInitializer:(0,o.Cx)(this.recurrentInitializer),biasInitializer:(0,o.Cx)(this.biasInitializer),unitForgetBias:this.unitForgetBias,kernelRegularizer:(0,c.SG)(this.kernelRegularizer),recurrentRegularizer:(0,c.SG)(this.recurrentRegularizer),biasRegularizer:(0,c.SG)(this.biasRegularizer),activityRegularizer:(0,c.SG)(this.activityRegularizer),kernelConstraint:(0,a.xF)(this.kernelConstraint),recurrentConstraint:(0,a.xF)(this.recurrentConstraint),biasConstraint:(0,a.xF)(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation};return Object.assign({},t,e)}}E.className="LSTMCell",r.m7h.registerClass(E);class N extends f{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new E(t),super(t)}call(t,e){return(0,r.lub)(()=>{null!=this.cell.dropoutMask&&(r.B90(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(r.B90(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);let i=null==e?null:e.mask,s=null==e?null:e.training,n=null==e?null:e.initialState;return super.call(t,{mask:i,training:s,initialState:n})})}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}N.className="LSTM",r.m7h.registerClass(N);class D extends k{constructor(t){super(t),this.cells=t.cells}get stateSize(){let t=[];for(let e of this.cells.slice().reverse())Array.isArray(e.stateSize)?t.push(...e.stateSize):t.push(e.stateSize);return t}call(t,e){return(0,r.lub)(()=>{let i=t.slice(1),r=[];for(let s of this.cells.slice().reverse())Array.isArray(s.stateSize)?r.push(i.splice(0,s.stateSize.length)):r.push(i.splice(0,1));r.reverse();let n=[],l;for(let a=0;a<this.cells.length;++a){let u=this.cells[a];i=r[a],l=0===a?[t[0]].concat(i):[l[0]].concat(i),l=u.call(l,e),n.push(l.slice(1))}for(let h of(i=[],n.slice().reverse()))i.push(...h);return[l[0]].concat(i)})}build(t){(0,d.XO)(t)&&(t=t[0]);let e;this.cells.forEach((i,r)=>{(0,l.f4)(`RNNCell_${r}`,()=>{i.build(t),e=Array.isArray(i.stateSize)?i.stateSize[0]:i.stateSize,t=[t[0],e]})}),this.built=!0}getConfig(){let t=super.getConfig(),e=t=>({className:t.getClassName(),config:t.getConfig()}),i=this.cells.map(e);return Object.assign({},t,{cells:i})}static fromConfig(t,e,i={}){let r=[];for(let s of e.cells)r.push((0,C.v)(s,i));return new t({cells:r})}get trainableWeights(){if(!this.trainable)return[];let t=[];for(let e of this.cells)t.push(...e.trainableWeights);return t}get nonTrainableWeights(){let t=[];for(let e of this.cells)t.push(...e.nonTrainableWeights);if(!this.trainable){let i=[];for(let r of this.cells)i.push(...r.trainableWeights);return i.concat(t)}return t}getWeights(){let t=[];for(let e of this.cells)t.push(...e.weights);return(0,I.FQ)(t)}setWeights(t){let e=[];for(let i of this.cells){let r=i.weights.length,s=t.splice(r);for(let n=0;n<i.weights.length;++n)e.push([i.weights[n],s[n]])}(0,I.zb)(e)}}function T(t){let{ones:e,rate:i,training:s=!1,count:l=1,dropoutFunc:a}=t,u=()=>null!=a?a(e(),i):n.rv(e(),i),h=()=>n.KC(u,e,s);if(!l||l<=1)return r.CnY(h().clone());let o=Array(l).fill(void 0).map(h);return o.map(t=>r.CnY(t.clone()))}D.className="StackedRNNCells",r.m7h.registerClass(D)}}]);